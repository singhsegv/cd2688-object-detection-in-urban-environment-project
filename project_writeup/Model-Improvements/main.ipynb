{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104055b2-3a7b-40f0-afb3-f75fc1ba9a96",
   "metadata": {},
   "source": [
    "# Model Improvements\n",
    "\n",
    "This notebook covers the accuracy improvement tasks mentioned in the **optional project rubrics**. I will describe few of the experiments I did to improve upon the Faster R-CNN model which was trained in the basic model section. I followed the following approach for this:\n",
    "1. Read the tensorboard output from its training and make a hypothesis for the loss or accuracy metric\n",
    "2. Make the required changes and retrain the model from scratch\n",
    "3. Check if the intended metrics got improved or not\n",
    "4. Repeat till the resources get over :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da40cc7-7262-4fda-968e-2d72fcbe1bc9",
   "metadata": {},
   "source": [
    "## 1. More Training Steps\n",
    "\n",
    "The first and easiest change that I tried was to train the model for more number of steps since the losses in the base model were showing downward trend till 2000 epochs. So, I trained the model for 4000 steps and compared the results.\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "Training the model should bring down the losses further and help model perform better than the base model.\n",
    "\n",
    "### Changes\n",
    "\n",
    "I changed the number of training steps from **2000** to **4000**. The learning rate scheduling and other hyper params were kept as they were.\n",
    "\n",
    "### Outcome\n",
    "\n",
    "* Training losses went down as expected.\n",
    "* Model started overfitting and performed worse in precision and recall metrics.\n",
    "\n",
    "**Precision Comparision**\n",
    "\n",
    "![mAP Comparision](./figures/steps_precision.png \"mAP\")\n",
    "\n",
    "**Recall Comparision**\n",
    "\n",
    "![Recall Comparision](./figures/steps_recall.png \"Recall\")\n",
    "\n",
    "**Losses Comparision**\n",
    "\n",
    "![Loss Comparision](./figures/steps_loss.png \"Loss\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7bb47f-71a2-4f50-89fc-153fd2cb5b4b",
   "metadata": {},
   "source": [
    "## 2. Image Augmentation\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "As I've mentioned in the exploratory data analysis write-up too that while exploring the type of scenes in the validation segments, I came across a few scenarios where:\n",
    "\n",
    "1. Sun caused video segment to have a super bright region and some glares on the camera\n",
    "2. Night time videos causing everything to look very dark\n",
    "\n",
    "Although training set also had very few scenes like this, I made a hypothesis that increasing such scenes using **brightness** augmentation should help the model to perform better since self-driving cars surely need to drive in such conditions some of the time.\n",
    "\n",
    "### Changes\n",
    "\n",
    "I added the below **random adjust brightness** augmentation with a max delta of 0.6 (which might have been too large now that I think of it) in the `faster_rcnn_pipeline.config` so that the images can replicate sunny and night scenes better.\n",
    "\n",
    "```\n",
    "data_augmentation_options {\n",
    "    random_adjust_brightness {\n",
    "      max_delta: 0.6\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Outcome\n",
    "\n",
    "I trained the model in the exact similar way as the basic trained model and compared the tensorboard metrics of the two.\n",
    "\n",
    "* The model where images were augmented performed slightly worse than the base model in all of the metrics.\n",
    "* My assumption is that it happened as both models were trained for same number of steps and the newer model had more variety of images to train on. So training it for more steps should make it better than the base model.\n",
    "* I didn't re-train it for more steps due to resource constraints for now. Will explore this later on.\n",
    "\n",
    "**Precision Comparision**\n",
    "\n",
    "![mAP Comparision](./figures/aug_precision.png \"mAP\")\n",
    "\n",
    "**Recall Comparision**\n",
    "\n",
    "![Recall Comparision](./figures/aug_recall.png \"Recall\")\n",
    "\n",
    "**Losses Comparision**\n",
    "\n",
    "![Loss Comparision](./figures/aug_loss.png \"Loss\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938383e5-ce15-40bd-85dd-a99020dcc1f7",
   "metadata": {},
   "source": [
    "## 3. Anchor Ratio Distribution\n",
    "\n",
    "The idea was that since the Region Proposal Network losses were adding a bigger amount to the total loss value, improvements around the anchors generation should help in improving the losses.\n",
    "\n",
    "So I planned on performing 2 steps:\n",
    "\n",
    "1. From the given training and validation data, plot a chart of distribution of aspect ratio (width to height ratios) of the ground truth boxes.\n",
    "2. Update the `first_stage_anchor_generator.grid_anchor_generator.aspect_ratios` value in the pipeline config.\n",
    "\n",
    "**Training Bounding Boxes aspect ratio**\n",
    "\n",
    "<img src=\"./figures/training_bbox_dist.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "**Validation Bounding Boxes aspect ratio**\n",
    "\n",
    "<img src=\"./figures/validation_bbox_dist.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "After analyzing the graphs, I came to this conclusion that both the training and validation sets have very similar distribution of bbox ratios. And putting efforts in this direction might help with the losses but won't help with the overfitting. So I didn't go forward in this direction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e8623-ffd8-482a-8f1a-b02fb91fd659",
   "metadata": {},
   "source": [
    "## Further Plans\n",
    "\n",
    "* Experiments with the optimizer needs to be done. This include trying different types of optimizers as well as different learning rates and annealing techniques on them. This should help in stabilizing the losses.\n",
    "* Spend some more time in figuring out what scenarios is the model not working well on.\n",
    "* Figuring out how to run evaluation after every step/epoch to figure out the trend and if overfitting is actually happening\n",
    "* Re-create the training and validation data with bigger dimensions and experiment with models with higher mAP and higher required image resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff6a8c-6692-4c76-8b91-aef0e58a73e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_object_detection] *",
   "language": "python",
   "name": "conda-env-tf_object_detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
